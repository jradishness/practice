{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naivebayes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RwWQlcGW5xY"
      },
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP3INry8XAHU"
      },
      "source": [
        "from collections import defaultdict, Counter\n",
        "import glob, math, operator, time\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9kULQ75Slj-"
      },
      "source": [
        "### DATA IMPORT\n",
        "The following cells load and unzip the training data directly from source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ficrh_sQIMn",
        "outputId": "1f44ee45-f9c9-4ad7-f1f3-872b35c94dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# get data from source\n",
        "!wget -nv \"http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-22 00:36:44 URL:http://www2.aueb.gr/users/ion/data/lingspam_public.tar.gz [11564714/11564714] -> \"lingspam_public.tar.gz\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H0WHqwRRjw2"
      },
      "source": [
        "# unzip data to memory\n",
        "!tar -xf lingspam_public.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_conXHrRWldg"
      },
      "source": [
        "### FUNCTIONS\n",
        "This is where the functions are constructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHS6etpJR0O6"
      },
      "source": [
        "def increment_nb_model(fname, model, vocab):\n",
        "  \"\"\"Function to process an email into either the \n",
        "  Ham or SPAM model.\n",
        "  \"\"\"\n",
        "  with open(fname) as email:   # open text file\n",
        "    lines = email.readlines()   # convert to py obj\n",
        "    for word in lines[2].split(\" \"):  # iterate through words(features)\n",
        "      model[word] += 1    # update model with count\n",
        "      vocab[word] += 1    # update vocab with count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqDJJmFYzl6"
      },
      "source": [
        "def train_nb_model(alpha=1):\n",
        "  \"\"\"Function to train Naive Bayes models.\n",
        "\n",
        "  Returns: logpriors(dict): set of prior probabilities for classes (0=ham; 1=spam)\n",
        "          loglikelihood(dict): dict (class-level) of dict of word likelihoods\n",
        "          vocab(list): set of Vocabulary\n",
        "  \"\"\"\n",
        "  vocab_dict = defaultdict(int) # initialize overall count dict\n",
        "  spam_dict = defaultdict(int) # initialize spam count dict\n",
        "  ham_dict = defaultdict(int) # initialize ham count dict\n",
        "  loglikelihood = defaultdict(lambda: defaultdict(float))  # initialize log likelihood dictionary\n",
        "  spamct, hamct = 0, 0  # initialize document counter for prior\n",
        "  \n",
        "  # Count all pertinent emails and features\n",
        "  for file in glob.glob(\"/content/lingspam_public/lemm_stop/part1/*\"):    # iterate through the pertinent folder\n",
        "    if file[41:].startswith(\"spms\"):  # determine spam or not\n",
        "      increment_nb_model(file, spam_dict, vocab_dict)  # update model with file\n",
        "      spamct += 1   # count file for NB calc\n",
        "    else: # or ham\n",
        "      increment_nb_model(file, ham_dict, vocab_dict)\n",
        "      hamct += 1\n",
        "\n",
        "  # Calculate Priors and Stats\n",
        "  spam_prior = math.log(spamct / (spamct + hamct)) # spam prior in log form\n",
        "  ham_prior = math.log(hamct / (spamct + hamct)) # ham prior in log form\n",
        "  logpriors = {\"ham\":ham_prior, \"spam\":spam_prior} # nest prior dicts for easy access\n",
        "  count_dict = {'spam':spam_dict, 'ham':ham_dict} # nest count dicts for easy access\n",
        "  token_ct_dict = {'spam':sum(spam_dict.values()), 'ham':sum(ham_dict.values())}\n",
        "  vocab = list(vocab_dict.keys()) # set of entire vocab\n",
        "  vocab_len = len(vocab) # size of vocab\n",
        "\n",
        "  # Calculate individual log probs\n",
        "  for x in ['spam', 'ham']:\n",
        "    for wordtype in vocab:\n",
        "      num = count_dict[x][wordtype] + alpha # calculate numerator\n",
        "      den = token_ct_dict[x] + (vocab_len * alpha)  # calculate denominator\n",
        "      loglikelihood[x][wordtype] = math.log(num / den)  # calculate (and store) log likelihood\n",
        "\n",
        "  return logpriors, loglikelihood, set(vocab) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiVub2FIVTHR"
      },
      "source": [
        "def nb_model_predict(fname, logpriors, loglikelihood, vocab, tags=['spam','ham']):\n",
        "  \"\"\"Function to predict a class label given proper input.\n",
        "  \"\"\"\n",
        "  pred = {}\n",
        "  for group in tags:\n",
        "    running_prob = logpriors[group]\n",
        "    with open(fname) as email:\n",
        "      lines = email.readlines()\n",
        "      for word in lines[2].split(\" \"):\n",
        "        if word in vocab:\n",
        "          running_prob += loglikelihood[group][word]\n",
        "    pred[group] = running_prob\n",
        "  return max(pred.items(), key=operator.itemgetter(1))[0]\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL6zAeGUmh_Y"
      },
      "source": [
        "### WORKFLOW\n",
        "Execution takes place here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sXCJSArXbJc",
        "outputId": "274be6d7-f8c7-4e41-d83c-8ab6d4540b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train Model\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model() # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.074 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOsNAsCYo_V",
        "outputId": "d763a0c6-62eb-442e-f96a-b05d33705f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# First Eval module: not through sklearn\n",
        "TP, FP, TN, FN = 0,0,0,0\n",
        "# fname = \"/content/lingspam_public/lemm_stop/part10/9-157msg1.txt\" # dev step\n",
        "# fname = \"/content/lingspam_public/lemm_stop/part10/spmsgc55.txt\" # dev step\n",
        "for fname in glob.glob(\"/content/lingspam_public/lemm_stop/part10/*\"):\n",
        "  pred = nb_model_predict(fname, logpriors, loglikelihood, vocab)\n",
        "  if \"spm\" in fname: # SPAM\n",
        "    if pred == \"spam\": # True Positives\n",
        "      TP += 1\n",
        "    elif pred == \"ham\": # False Negatives\n",
        "      FN += 1\n",
        "    else: \n",
        "      print(\"ERROR: Spam predicted as something other than Spam or Ham\")\n",
        "  else:  # HAM\n",
        "    if pred == \"ham\": # True Negatives\n",
        "      TN += 1\n",
        "    elif pred == \"spam\": # False Positives\n",
        "      FP += 1 \n",
        "    else:\n",
        "      print(\"ERROR: Ham predicted as something other than Spam or Ham\")\n",
        "\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "fmeasure = (2 * precision * recall) / (precision + recall)\n",
        "print(\"Precision: {}\\tRecall: {}\\t\\tF1:{}\\n\".format(round(precision,3),round(recall,3),round(fmeasure,4)))\n",
        "print(\"Confusion Mat. | Positive Prediction \\t| Negative Prediction\")\n",
        "print(\"Positive Class | True Positives {}\\t| False Negatives {}\".format(TP, FN))\n",
        "print(\"Negative Class | False Positives {}\\t| True Negatives {}\".format(FP, TN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.754\tRecall: 0.878\t\tF1:0.8113\n",
            "\n",
            "Confusion Mat. | Positive Prediction \t| Negative Prediction\n",
            "Positive Class | True Positives 43\t| False Negatives 6\n",
            "Negative Class | False Positives 14\t| True Negatives 228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TFV1RckrEJ"
      },
      "source": [
        "Evaluation: What is the Precision, Recall, and F-score of the classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmbyVPHkpFR"
      },
      "source": [
        "def run_predictions():\n",
        "  t2 = time.time()\n",
        "  preds = []\n",
        "  gold = []\n",
        "  # fname = \"/content/lingspam_public/lemm_stop/part10/9-157msg1.txt\" # dev step\n",
        "  # fname = \"/content/lingspam_public/lemm_stop/part10/spmsgc55.txt\" # dev step\n",
        "  for fname in glob.glob(\"/content/lingspam_public/lemm_stop/part10/*\"):\n",
        "    preds.append(nb_model_predict(fname, logpriors, loglikelihood, vocab))\n",
        "    if \"spm\" in fname: # SPAM Gold\n",
        "      gold.append(\"spam\")\n",
        "    else:   # Ham Gold\n",
        "      gold.append(\"ham\")\n",
        "\n",
        "  y_true = np.array(gold)\n",
        "  y_pred = np.array(preds)\n",
        "  # precision_recall_fscore_support(y_true, y_pred)\n",
        "  stats = precision_recall_fscore_support(y_true, y_pred, average=None, labels=['ham', 'spam'])\n",
        "  names = [\"Precision\", \"Recall\\t\", \"Fscore\\t\", \"Support\\t\"]\n",
        "  print(\" \\t\\tHam\\t\\tSpam\")\n",
        "  for i, stat in enumerate(stats):\n",
        "    print(\"{}\\t{}\".format(names[i], stat))\n",
        "  print(\"Time to Test: {} seconds\".format(round(time.time()-t2, 3)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfDlO1U3k3Rq"
      },
      "source": [
        "### Experimentation with Smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6O50Gz-lBg7",
        "outputId": "6f63e3e0-534e-436f-f704-a3d29d2db42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=1 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=1) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.091 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.97435897 0.75438596]\n",
            "Recall\t\t[0.94214876 0.87755102]\n",
            "Fscore\t\t[0.95798319 0.81132075]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.121 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bsdh6z-tBlF",
        "outputId": "c11c5faf-6128-488b-c6bc-99e9b3425e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.5 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.5) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.076 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.97435897 0.75438596]\n",
            "Recall\t\t[0.94214876 0.87755102]\n",
            "Fscore\t\t[0.95798319 0.81132075]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.105 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaZ92Ycnr6g_",
        "outputId": "34788470-6359-48c3-ec8a-363f07b0f555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.1 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.1) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.071 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.97435897 0.75438596]\n",
            "Recall\t\t[0.94214876 0.87755102]\n",
            "Fscore\t\t[0.95798319 0.81132075]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.114 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waHmC52-suOk"
      },
      "source": [
        "Changing the alpha from 1, to .9 or .8, or anything down to .1 had no effect on system performance. However, taking it below .1 began to improve preformance on both categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-2wl0Bvs_yE",
        "outputId": "fc1053b7-c092-4fa4-c4ca-41698cbf8d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.01 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.01) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.076 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.97468354 0.7962963 ]\n",
            "Recall\t\t[0.95454545 0.87755102]\n",
            "Fscore\t\t[0.96450939 0.83495146]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.119 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAY3KRC0thKY"
      },
      "source": [
        "At .001, we get diminishing returns from the Recall on Spam and precision on Ham, while everything else (including F1) has improved slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieeg47octbug",
        "outputId": "2f31a35d-fe6f-4497-bfc8-807c423ff86c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.001 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.001) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.081 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.96326531 0.86956522]\n",
            "Recall\t\t[0.97520661 0.81632653]\n",
            "Fscore\t\t[0.96919918 0.84210526]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.119 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSI6JunPvWK8"
      },
      "source": [
        "Interestingly, I was expecting the performance of alpha=0.0055 to fall between a=0.001 and a=0.001, but it's simply bad. So, I think I would stick with .001 in practice. It's the only parameter value that brings all recall and precision values above 80%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwNxWWvGt36B",
        "outputId": "bf2935c2-bb78-41ef-a4c9-3bb2caf747e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.0055 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.0055) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.074 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.97058824 0.79245283]\n",
            "Recall\t\t[0.95454545 0.85714286]\n",
            "Fscore\t\t[0.9625     0.82352941]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.11 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJRwufhXvt88",
        "outputId": "0b8df451-56b5-42d1-8738-9114764dceb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train Model with alpha=0.0001 smoothing\n",
        "t1 = time.time()\n",
        "logpriors, loglikelihood, vocab = train_nb_model(alpha=0.0001) # train model\n",
        "print(\"Time to Train: {} seconds\".format(round(time.time()-t1, 3)))\n",
        "run_predictions() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to Train: 0.085 seconds\n",
            " \t\tHam\t\tSpam\n",
            "Precision\t[0.95951417 0.88636364]\n",
            "Recall\t\t[0.97933884 0.79591837]\n",
            "Fscore\t\t[0.96932515 0.83870968]\n",
            "Support\t\t[242  49]\n",
            "Time to Test: 0.103 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hliwt3iplE7Y"
      },
      "source": [
        "Classifier comparison with sklearn's implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfvJbiQ1lFQ4"
      },
      "source": [
        "# Process emails\n",
        "def process_email(fname):\n",
        "  with open(fname) as email:   # open text file\n",
        "    lines = email.readlines()   # convert to py obj\n",
        "  return lines[2].strip()\n",
        "\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for fname in glob.glob(\"/content/lingspam_public/lemm_stop/part1/*\"):    # iterate through the pertinent folder\n",
        "  if fname[41:].startswith(\"spms\"):  # determine spam or not\n",
        "    train_labels.append(\"spam\")\n",
        "  else:\n",
        "    train_labels.append(\"ham\")\n",
        "  train_texts.append(process_email(fname))\n",
        "\n",
        "# Count Vectorizer Step\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(train_texts)\n",
        "\n",
        "# Train MnNB Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_counts, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6LOjJss9iAG",
        "outputId": "2fbd165f-4037-4bd9-b42b-507c5d438407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Counter(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'ham': 241, 'spam': 48})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePgT-9Qu5NHl"
      },
      "source": [
        "# Process Test Set\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for fname in glob.glob(\"/content/lingspam_public/lemm_stop/part10/*\"):\n",
        "  if \"spms\" in fname:  # determine spam or not\n",
        "    test_labels.append(\"spam\")\n",
        "  else:\n",
        "    test_labels.append(\"ham\")\n",
        "  test_texts.append(process_email(fname))\n",
        "\n",
        "# Transform Features\n",
        "X_new_counts = count_vect.transform(test_texts)\n",
        "# X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "# Make predictions with Trained Multinomial NB from SKLearn\n",
        "predicted = clf.predict(X_new_counts)\n",
        "\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(predicted)\n",
        "\n",
        "\n",
        "# stats = precision_recall_fscore_support(y_true, y_pred, average=None, labels=['ham', 'spam'])\n",
        "# names = [\"Precision\", \"Recall\\t\", \"Fscore\\t\", \"Support\\t\"]\n",
        "# print(\" \\t\\tHam\\t\\tSpam\")\n",
        "# for i, stat in enumerate(stats):\n",
        "#   print(\"{}\\t{}\".format(names[i], stat))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEh9fl90SZ-q",
        "outputId": "002654d9-b057-4aa6-8ab4-feeb30ec5e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_true, y_pred, \n",
        "                                    target_names=list(set(train_labels))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      1.00      0.99       242\n",
            "        spam       1.00      0.94      0.97        49\n",
            "\n",
            "    accuracy                           0.99       291\n",
            "   macro avg       0.99      0.97      0.98       291\n",
            "weighted avg       0.99      0.99      0.99       291\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXvRe-4JW_zP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}